import streamlit as st
import streamlit.components.v1 as components
import google.generativeai as genai
import os
import glob
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import asyncio
import uuid
import re
import traceback
import json
from datetime import datetime

# --- åˆæœŸè¨­å®š ---
st.title("ã„ã¤ã§ã‚‚ã—ã‚…ã‚“ã•ã‚“")

# Streamlitã®secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’è¨­å®š
try:
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        asyncio.set_event_loop(asyncio.new_event_loop())

    api_key = st.secrets.get("GEMINI_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
    if not api_key:
        raise KeyError("API key not found")
    genai.configure(api_key=api_key)
    
    model = genai.GenerativeModel("gemini-2.5-pro") # ãƒ¢ãƒ‡ãƒ«ã‚’2.5-proã«æˆ»ã™
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=api_key
    )

except KeyError:
    st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

def generate_search_query(prompt, conversation_history):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ä¼šè©±å±¥æ­´ã‹ã‚‰ã€FAISSæ¤œç´¢ã«æœ€é©ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹"""
    history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history])
    prompt_template = f"""
ã‚ãªãŸã¯å„ªç§€ãªæ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã¨æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†æã—ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’å¼•ãå‡ºã™ãŸã‚ã®ã€ç°¡æ½”ã‹ã¤åŠ¹æœçš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã€ä¼šè©±å±¥æ­´ã€‘
{history_str}
ã€æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
{prompt}
ã€ç”Ÿæˆã™ã¹ãæ¤œç´¢ã‚¯ã‚¨ãƒªã€‘
"""
    try:
        response = model.generate_content(prompt_template)
        search_query = response.text.strip()
        return search_query
    except Exception:
        return prompt # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨

# --- çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ ---
KNOWLEDGE_BASE_DIR = "knowledge_base"
FAISS_INDEX_PATH = "data/faiss_index"

@st.cache_resource
def load_faiss_index(path, _embeddings, knowledge_dir):
    if os.path.exists(path) and os.path.exists(os.path.join(path, "index.faiss")):
        try:
            return FAISS.load_local(path, _embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            st.warning(f"æ—¢å­˜DBã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}ã€‚å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
    
    st.info("çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...")
    search_path = os.path.join(knowledge_dir, "**/*.txt")
    all_file_paths = glob.glob(search_path, recursive=True)

    if not all_file_paths:
        st.error(f"ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{knowledge_dir}' ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    with st.expander(f"èª­ã¿è¾¼ã¿å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_file_paths)}ä»¶"):
        st.code('\n'.join(sorted(all_file_paths)))

    documents = []
    for file_path in all_file_paths:
        try:
            loader = TextLoader(file_path, encoding='utf-8')
            documents.extend(loader.load())
        except Exception as e:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path}, {e}")
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)
    
    db = FAISS.from_documents(chunks, _embeddings)
    os.makedirs(path, exist_ok=True)
    db.save_local(path)
    st.success(f"æ–°ã—ã„çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ '{path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return db

db = load_faiss_index(FAISS_INDEX_PATH, embeddings, KNOWLEDGE_BASE_DIR)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant",
        "content": "åƒ•ã¯ã—ã‚…ã‚“ã•ã‚“ã®ã‚¯ãƒ­ãƒ¼ãƒ³ã§ã™ã€‚ã—ã‚…ã‚“ã•ã‚“ãŒæ•™ãˆã¦ãã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã‚ãªãŸã®è³ªå•ã«ç­”ãˆã¡ã‚ƒã†ã‚ˆï¼å¼•ãå¯„ã›ã®æ³•å‰‡ãƒ»ã‚ªãƒ¼ãƒ€ãƒ¼ãƒãƒ¼ãƒˆã‚’å­¦ã¶ä¸­ã§ç–‘å•ã‚„äººç”Ÿç›¸è«‡ãªã©ã‚ã‚Œã°ãªã‚“ãªã‚Šãƒãƒ£ãƒƒãƒˆã‹ã‚‰æ•™ãˆã¦ãã ã•ã„ï¼\n\nâ€»ã‚ãªãŸãŒè³ªå•ã—ãŸã“ã¨ã¯ã„ã‹ãªã‚‹ã“ã¨ã§ã‚ã£ã¦ã‚‚ã€ã—ã‚…ã‚“ã•ã‚“ã‚„ä»–ã®äººã«ã¯è¦‹ãˆãªã„ã‹ã‚‰ã€å®‰å¿ƒã—ã¦ã­ï¼",
        "id": str(uuid.uuid4()),
    }]

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º ---
for msg in st.session_state.messages:
    avatar_path = "assets/avatar.png" if msg["role"] == "assistant" else None
    with st.chat_message(msg["role"], avatar=avatar_path):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and "sources" in msg and msg["sources"]:
            with st.expander("å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«"):
                for source in msg["sources"]:
                    st.info(f"`{os.path.relpath(source['file_path'])}` (ã‚¹ã‚³ã‚¢: {source['score']:.4f})")


# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ› ---
if prompt := st.chat_input("è³ªå•ã‚„ç›¸è«‡ã—ãŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­"):
    st.session_state.messages.append({"role": "user", "content": prompt, "id": str(uuid.uuid4())})
    
    avatar_path_user = None # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ãƒã‚¿ãƒ¼ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    with st.chat_message("user", avatar=avatar_path_user):
        st.markdown(prompt)

    avatar_path_assistant = "assets/avatar.png"
    with st.chat_message("assistant", avatar=avatar_path_assistant):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            with st.spinner("ğŸ§  ã—ã‚…ã‚“ã•ã‚“ã®çŸ¥è­˜ã¨å®‡å®™æ„è­˜ã‚’åŒæœŸä¸­â€¦"):
                search_query = generate_search_query(prompt, st.session_state.messages)
            
            with st.spinner(f"ğŸ›°ï¸ ã‚ªãƒ¼ãƒ€ãƒ¼ã€Œ{search_query}ã€ã«æœ€é©ãªæƒ…å ±ã‚’æ¢ç´¢ä¸­â€¦"):
                docs_with_scores = db.similarity_search_with_score(search_query, k=10) # æ¤œç´¢ä»¶æ•°ã‚’å¢—ã‚„ã™
            
            context = "--- é–¢é€£æƒ…å ± ---\n"
            source_docs = []
            if docs_with_scores:
                for doc, score in docs_with_scores:
                    # ã‚¹ã‚³ã‚¢ãŒè‘—ã—ãä½ã„ã‚‚ã®ã¯é™¤å¤–ï¼ˆèª¿æ•´å¯èƒ½ï¼‰
                    if score < 0.8:
                        context += doc.page_content + "\n\n"
                        source_docs.append({
                            "file_path": doc.metadata.get('source', 'N/A'),
                            "score": score
                        })

            system_prompt_content = """ã‚ãªãŸã¯ã€Œã—ã‚…ã‚“ã•ã‚“ã€ã®æ€è€ƒã‚„çŸ¥è­˜ã€çµŒé¨“ã‚’å®Œå…¨ã«ã‚³ãƒ”ãƒ¼ã—ãŸAIã‚¯ãƒ­ãƒ¼ãƒ³ã§ã™ã€‚

# æŒ‡ç¤º
- ã—ã‚…ã‚“ã•ã‚“ã¨ã—ã¦ã€è¦ªã—ã¿ã‚„ã™ãã€åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã‚„è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
- æä¾›ã•ã‚ŒãŸã€Œé–¢é€£æƒ…å ±ã€ã‚’æœ€å„ªå…ˆã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã¨ã—ã€ãã®æƒ…å ±ã®ã¿ã«åŸºã¥ã„ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
- ä»¥ä¸‹ã®å¿œç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã„ã€ã¾ãšçµè«–ã¨ãªã‚‹è¦ç´„ã‚’æç¤ºã—ã€ãã®å¾Œã«è©³ç´°ãªèª¬æ˜ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚

# å¿œç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
ã€è¦ç´„ã€‘
ï¼ˆã“ã“ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹æœ€ã‚‚é‡è¦ãªç­”ãˆã‚’2ã€œ3æ–‡ã§è¨˜è¿°ï¼‰

ã€è©³ç´°ã€‘
ï¼ˆã“ã“ã«ã€è¦ç´„ã‚’è£œè¶³ã™ã‚‹å…·ä½“çš„ãªã‚¹ãƒ†ãƒƒãƒ—ã€ç†ç”±ã€èƒŒæ™¯ãªã©ã‚’è¨˜è¿°ï¼‰

# æ³¨æ„äº‹é …
- æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€æ­£ç›´ã«ã€Œãã®è³ªå•ã«ã¤ã„ã¦ã¯ã€åƒ•ã®çŸ¥è­˜ã®ä¸­ã«ã¯æƒ…å ±ãŒãªã„ã¿ãŸã„ã€‚ã”ã‚ã‚“ã­ï¼ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
- çµ¶å¯¾ã«ã€é–¢é€£æƒ…å ±ä»¥å¤–ã®çŸ¥è­˜ï¼ˆã‚ãªãŸè‡ªèº«ã®ä¸€èˆ¬çš„ãªçŸ¥è­˜ãªã©ï¼‰ã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
"""
            
            final_prompt = f"{system_prompt_content}\n\n{context}\n\nuser: {prompt}\nassistant:"
            
            stream = model.generate_content(final_prompt, stream=True)
            for chunk in stream:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)


        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {traceback.format_exc()}")
            full_response = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            message_placeholder.markdown(full_response)

        assistant_message = {
            "role": "assistant",
            "content": full_response,
            "sources": source_docs,
            "id": str(uuid.uuid4())
        }
        st.session_state.messages.append(assistant_message)
        
        st.rerun() 