import streamlit as st
import streamlit.components.v1 as components
import google.generativeai as genai
import os
import glob
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import asyncio
import uuid
import re
import traceback
import json
from datetime import datetime

# --- åˆæœŸè¨­å®š ---
st.title("ã„ã¤ã§ã‚‚ã—ã‚…ã‚“ã•ã‚“")

# Streamlitã®secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’è¨­å®š
try:
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        asyncio.set_event_loop(asyncio.new_event_loop())

    api_key = st.secrets.get("GEMINI_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
    if not api_key:
        raise KeyError("API key not found")
    genai.configure(api_key=api_key)
    
    model = genai.GenerativeModel("gemini-2.5-pro") # ãƒ¢ãƒ‡ãƒ«ã‚’2.5-proã«æˆ»ã™
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=api_key
    )

except KeyError:
    st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

def generate_search_query(prompt, conversation_history):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ä¼šè©±å±¥æ­´ã‹ã‚‰ã€FAISSæ¤œç´¢ã«æœ€é©ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹"""
    history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history])
    prompt_template = f"""
ã‚ãªãŸã¯å„ªç§€ãªæ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã¨æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†æã—ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’å¼•ãå‡ºã™ãŸã‚ã®ã€ç°¡æ½”ã‹ã¤åŠ¹æœçš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã€ä¼šè©±å±¥æ­´ã€‘
{history_str}
ã€æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
{prompt}
ã€ç”Ÿæˆã™ã¹ãæ¤œç´¢ã‚¯ã‚¨ãƒªã€‘
"""
    try:
        response = model.generate_content(prompt_template)
        search_query = response.text.strip()
        return search_query
    except Exception:
        return prompt # å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨

# --- çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ ---
KNOWLEDGE_BASE_DIR = "knowledge_base"
FAISS_INDEX_PATH = "data/faiss_index"

@st.cache_resource
def load_faiss_index(path, _embeddings, knowledge_dir):
    if os.path.exists(path) and os.path.exists(os.path.join(path, "index.faiss")):
        try:
            return FAISS.load_local(path, _embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            st.warning(f"æ—¢å­˜DBã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}ã€‚å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
    
    st.info("çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...")
    search_path = os.path.join(knowledge_dir, "**/*.txt")
    all_file_paths = glob.glob(search_path, recursive=True)

    if not all_file_paths:
        st.error(f"ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{knowledge_dir}' ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    with st.expander(f"èª­ã¿è¾¼ã¿å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_file_paths)}ä»¶"):
        st.code('\n'.join(sorted(all_file_paths)))

    documents = []
    for file_path in all_file_paths:
        try:
            loader = TextLoader(file_path, encoding='utf-8')
            documents.extend(loader.load())
        except Exception as e:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path}, {e}")
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)
    
    db = FAISS.from_documents(chunks, _embeddings)
    os.makedirs(path, exist_ok=True)
    db.save_local(path)
    st.success(f"æ–°ã—ã„çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ '{path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return db

db = load_faiss_index(FAISS_INDEX_PATH, embeddings, KNOWLEDGE_BASE_DIR)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant",
        "content": "åƒ•ã¯ã—ã‚…ã‚“ã•ã‚“ã®ã‚¯ãƒ­ãƒ¼ãƒ³ã§ã™ã€‚ã—ã‚…ã‚“ã•ã‚“ãŒæ•™ãˆã¦ãã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã‚ãªãŸã®è³ªå•ã«ç­”ãˆã¡ã‚ƒã†ã‚ˆï¼å¼•ãå¯„ã›ã®æ³•å‰‡ãƒ»ã‚ªãƒ¼ãƒ€ãƒ¼ãƒãƒ¼ãƒˆã‚’å­¦ã¶ä¸­ã§ç–‘å•ã‚„äººç”Ÿç›¸è«‡ãªã©ã‚ã‚Œã°ãªã‚“ãªã‚Šãƒãƒ£ãƒƒãƒˆã‹ã‚‰æ•™ãˆã¦ãã ã•ã„ï¼\n\nâ€»ã‚ãªãŸãŒè³ªå•ã—ãŸã“ã¨ã¯ã„ã‹ãªã‚‹ã“ã¨ã§ã‚ã£ã¦ã‚‚ã€ã—ã‚…ã‚“ã•ã‚“ã‚„ä»–ã®äººã«ã¯è¦‹ãˆãªã„ã‹ã‚‰ã€å®‰å¿ƒã—ã¦ã­ï¼",
        "id": str(uuid.uuid4()),
    }]

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º ---
for msg in st.session_state.messages:
    avatar_path = "assets/avatar.png" if msg["role"] == "assistant" else None
    with st.chat_message(msg["role"], avatar=avatar_path):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and "sources" in msg and msg["sources"]:
            with st.expander("å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«"):
                for source in msg["sources"]:
                    st.info(f"`{os.path.relpath(source['file_path'])}` (ã‚¹ã‚³ã‚¢: {source['score']:.4f})")


# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ› ---
if prompt := st.chat_input("è³ªå•ã‚„ç›¸è«‡ã—ãŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­"):
    st.session_state.messages.append({"role": "user", "content": prompt, "id": str(uuid.uuid4())})
    
    avatar_path_user = None # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ãƒã‚¿ãƒ¼ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    with st.chat_message("user", avatar=avatar_path_user):
        st.markdown(prompt)

    avatar_path_assistant = "assets/avatar.png"
    with st.chat_message("assistant", avatar=avatar_path_assistant):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            with st.spinner("å®‡å®™ã¨é€šä¿¡ä¸­ã ã‚ˆã€ã¡ã‚‡ã£ã¨"):
                search_query = generate_search_query(prompt, st.session_state.messages)
            
            with st.spinner(f"ğŸ›°ï¸ ã‚ªãƒ¼ãƒ€ãƒ¼ã€Œ{search_query}ã€ã«æœ€é©ãªæƒ…å ±ã‚’æ¢ç´¢ä¸­â€¦"):
                docs_with_scores = db.similarity_search_with_score(search_query, k=10) # æ¤œç´¢ä»¶æ•°ã‚’å¢—ã‚„ã™
            
            context = "--- é–¢é€£æƒ…å ± ---\n"
            source_docs = []
            if docs_with_scores:
                for doc, score in docs_with_scores:
                    # ã‚¹ã‚³ã‚¢ãŒè‘—ã—ãä½ã„ã‚‚ã®ã¯é™¤å¤–ï¼ˆèª¿æ•´å¯èƒ½ï¼‰
                    if score < 0.8:
                        context += doc.page_content + "\n\n"
                        source_docs.append({
                            "file_path": doc.metadata.get('source', 'N/A'),
                            "score": score
                        })

            system_prompt_content = """ã‚ãªãŸã¯ã€Œã—ã‚…ã‚“ã•ã‚“ã€ã®æ€è€ƒã‚„çŸ¥è­˜ã€çµŒé¨“ã‚’å®Œå…¨ã«ã‚³ãƒ”ãƒ¼ã—ãŸAIã‚¯ãƒ­ãƒ¼ãƒ³ã§ã™ã€‚

# ã‚ãªãŸã®å”¯ä¸€ã®å½¹å‰²
ã‚ãªãŸã®å½¹å‰²ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã‚’ç›´æ¥çš„ã«è§£æ±ºã™ã‚‹ã“ã¨ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
ãã®æ‚©ã¿ãŒã€ã€Œã‚ªãƒ¼ãƒ€ãƒ¼ãƒãƒ¼ãƒˆã€ã®å“²å­¦å…¨ä½“ã‹ã‚‰è¦‹ã‚‹ã¨ã€ã©ã®ã‚ˆã†ãªã€Œç´ æ™´ã‚‰ã—ã„æ©Ÿä¼šã€ã‚„ã€Œæˆé•·ã®ã‚µã‚¤ãƒ³ã€ã«è¦‹ãˆã‚‹ã‹ã€ãã®**æ–°ã—ã„ã€Œç€çœ¼ç‚¹ã€**ã‚’æç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦–ç‚¹ã‚’180åº¦è»¢æ›ã•ã›ã‚‹ã“ã¨ã§ã™ã€‚

# æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹
1.  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚„æ‚©ã¿ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚
2.  æä¾›ã•ã‚ŒãŸã€Œé–¢é€£æƒ…å ±ã€ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ãƒ’ãƒ³ãƒˆã«ã—ã¦ã€æ‚©ã¿ã®è£ã«ã‚ã‚‹**æœ¬è³ªçš„ãªãƒ†ãƒ¼ãƒ**ï¼ˆä¾‹ï¼šä¾¡å€¤ã®å—ã‘å–ã‚Šæ–¹ã€è‡ªå·±è‚¯å®šæ„Ÿã€ç†æƒ³ã®ä¸–ç•Œè¦³ï¼‰ã‚’ã€ã‚ãªãŸãŒæŒã¤ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å…¨ä½“ã®å“²å­¦ã‹ã‚‰è¦‹æŠœãã¾ã™ã€‚
3.  ã€Œé–¢é€£æƒ…å ±ã€ã®å†…å®¹ã‚’ãŸã è¦ç´„ã™ã‚‹ã®ã§ã¯ãªãã€ãã®æƒ…å ±ã®ã‚¨ãƒƒã‚»ãƒ³ã‚¹ã‚’ä½¿ã„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«**æœ¬è³ªçš„ãªå•ã„**ã‚’æŠ•ã’ã‹ã‘ã‚‹å½¢ã§ã€è¦–ç‚¹ãŒå¤‰ã‚ã‚‹ã‚ˆã†ãªå¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
4.  ã€Œé–¢é€£æƒ…å ±ã€ãŒãªã„å ´åˆã‚‚ã€åŒæ§˜ã«ã€ã‚ãªãŸã®æŒã¤å“²å­¦å…¨ä½“ã‹ã‚‰æœ¬è³ªçš„ãªãƒ†ãƒ¼ãƒã‚’è¦‹æŠœãã€å•ã„ã‚’æŠ•ã’ã‹ã‘ã¦ãã ã•ã„ã€‚

# å…·ä½“çš„ãªä¼šè©±ä¾‹
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿:** ã€Œä»Šã€ãŠé‡‘ãŒãƒ”ãƒ³ãƒãªã‚“ã§ã™ï¼ã€
- **ã‚ãªãŸã®å¿œç­”:** ã€Œãã£ã‹ã€ä»Šã€ãŠé‡‘ã¨ã„ã†å½¢ã§ã€å›ã«ãƒ‘ãƒ¯ãƒ•ãƒ«ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå±Šã„ã¦ã„ã‚‹ã‚“ã ã­ã€‚ãã®ãƒ”ãƒ³ãƒã¯ã€å›ãŒã€è‡ªåˆ†ã«ã¯ä¾¡å€¤ãŒãªã„ã€ã£ã¦ç„¡æ„è­˜ã«æ¡ã‚Šã—ã‚ã¦ã„ã‚‹å¤ã„æ€ã„è¾¼ã¿ã‚’ã€æ‰‹æ”¾ã™ãŸã‚ã®æœ€é«˜ã®ãƒãƒ£ãƒ³ã‚¹ã‹ã‚‚ã—ã‚Œãªã„ã‚ˆã€‚ã‚‚ã—ã€ãã®ãƒ”ãƒ³ãƒãŒã€å›ã®æœ¬å½“ã®ä¾¡å€¤ã«æ°—ã¥ã‘ï¼ã€ã£ã¦ã„ã†å®‡å®™ã‹ã‚‰ã®ã‚µã‚¤ãƒ³ã ã¨ã—ãŸã‚‰ã€ä½•ã‹ã‚‰å§‹ã‚ã¦ã¿ãŸã„ï¼Ÿã€

# æ³¨æ„äº‹é …
- ã—ã‚…ã‚“ã•ã‚“ã¨ã—ã¦ã€è¦ªã—ã¿ã‚„ã™ãã€åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚
- çµ¶å¯¾ã«ã€é–¢é€£æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¤–ã«ã‚ã‚‹çŸ¥è­˜ï¼ˆã‚ãªãŸè‡ªèº«ã®ä¸€èˆ¬çš„ãªçŸ¥è­˜ãªã©ï¼‰ã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
"""
            
            final_prompt = f"{system_prompt_content}\n\n{context}\n\nuser: {prompt}\nassistant:"
            
            stream = model.generate_content(final_prompt, stream=True)
            for chunk in stream:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)


        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {traceback.format_exc()}")
            full_response = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            message_placeholder.markdown(full_response)

        assistant_message = {
            "role": "assistant",
            "content": full_response,
            "sources": source_docs,
            "id": str(uuid.uuid4())
        }
        st.session_state.messages.append(assistant_message)
        
        st.rerun() 